import streamlit as st
import whisper

# import openai
import os
from pydub import AudioSegment
from datetime import timedelta
import tempfile
from dotenv import load_dotenv
import requests
import json

# ========== ENV ==========
load_dotenv()
# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
# openai.api_key = OPENAI_API_KEY

# ========== CONFIG ==========
# MODEL_SIZE = "medium"  # Whisper model size: "tiny", "base", "small", "medium", "large"
# CHUNK_DURATION_MINUTES = 10
# # ===========================

# client = openai.OpenAI(api_key=OPENAI_API_KEY)


def format_time(ms):
    return str(timedelta(milliseconds=ms))


def split_audio(audio, chunk_duration_ms):
    return [
        audio[i : i + chunk_duration_ms]
        for i in range(0, len(audio), chunk_duration_ms)
    ]


def transcribe_audio(audio_file, model, index=None):
    label = f"chunk {index}" if index is not None else "file"
    st.info(f"ğŸ”Š Transcribing {label}...")
    result = model.transcribe(audio_file, language="fa")
    return result["text"]


def summarize_text(transcript):
    st.info("ğŸ§  Summarizing the transcription...")

    prompt = f"""Ù…ØªÙ† Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø®Ù„Ø§ØµÙ‡ Ùˆ Ø±ÙˆØ§Ù† Ø¯Ø± Ú†Ù†Ø¯ Ø¬Ù…Ù„Ù‡ Ø¨ÛŒØ§Ù† Ú©Ù†:\n\n{transcript}"""

    payload = {
        "model": "llama3",  # or any model you've pulled
        "messages": [
            {
                "role": "system",
                "content": "ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡Ø³ØªÛŒ Ú©Ù‡ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø®Ù„Ø§ØµÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒ.",
            },
            {"role": "user", "content": prompt},
        ],
        "stream": False,
    }

    try:
        response = requests.post("http://localhost:11434/api/chat", json=payload)
        response.raise_for_status()
        summary = response.json()["message"]["content"]
        return summary.strip()

    except requests.exceptions.RequestException as e:
        st.error("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ù…Ø¯Ù„ Ollama")
        st.error(str(e))
        return "Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯."


def main():
    st.set_page_config(page_title="Persian Audio Summarizer", layout="centered")
    st.title("ğŸ§  Persian Audio Summarizer")
    st.markdown("Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ ÙØ§Ø±Ø³ÛŒ Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø¢Ù† Ø¨Ø§ Ú©Ù…Ú© Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ")

    uploaded_file = st.file_uploader(
        "ğŸ™ï¸ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯", type=["mp3", "wav", "m4a"]
    )

    if uploaded_file is not None:
        # ğŸŸ¢ Step 1: Save uploaded file temporarily
        with st.spinner("ğŸ“ Ø¯Ø± Ø­Ø§Ù„ Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ..."):
            with tempfile.NamedTemporaryFile(
                delete=False, suffix=uploaded_file.name[-4:]
            ) as tmp_file:
                tmp_file.write(uploaded_file.read())
                temp_path = tmp_file.name

        st.success("âœ… ÙØ§ÛŒÙ„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!")

        # ğŸ§ Step 2: Load & preprocess audio
        with st.spinner("ğŸšï¸ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ..."):
            audio = (
                AudioSegment.from_file(temp_path).set_frame_rate(16000).set_channels(1)
            )

        # âœ‚ï¸ Step 3: Split audio if too long
        chunk_duration_ms = CHUNK_DURATION_MINUTES * 60 * 1000
        if len(audio) > chunk_duration_ms:
            st.info(f"ğŸ§ Ø·ÙˆÙ„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø²ÛŒØ§Ø¯ Ø§Ø³Øª: {format_time(len(audio))}")
            with st.spinner("âœ‚ï¸ Ø¯Ø± Ø­Ø§Ù„ ØªÙ‚Ø³ÛŒÙ… ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©ØªØ±..."):
                chunks = split_audio(audio, chunk_duration_ms)
            st.success(f"âœ… ÙØ§ÛŒÙ„ Ø¨Ù‡ {len(chunks)} Ø¨Ø®Ø´ ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯.")
        else:
            chunks = [audio]

        # ğŸ“¦ Step 4: Load Whisper model
        with st.spinner("ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Whisper..."):
            whisper_model = whisper.load_model(MODEL_SIZE)

        # ğŸ”Š Step 5: Transcribe chunks
        full_transcript = ""
        progress_bar = st.progress(0, text="ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ†...")

        for idx, chunk in enumerate(chunks):
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_chunk:
                chunk.export(temp_chunk.name, format="wav")

                with st.spinner(f"ğŸ”Š Ø¨Ø®Ø´ {idx+1} Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø³Øª..."):
                    text = transcribe_audio(
                        temp_chunk.name, whisper_model, index=idx + 1
                    )

                full_transcript += text + "\n"
                os.remove(temp_chunk.name)

            progress = int(((idx + 1) / len(chunks)) * 100)
            progress_bar.progress(
                progress, text=f"ğŸ“¦ Ù¾Ø±Ø¯Ø§Ø²Ø´ {idx + 1} Ø§Ø² {len(chunks)} Ø¨Ø®Ø´"
            )

        progress_bar.empty()

        # ğŸ“„ Step 6: Show transcript
        st.subheader("ğŸ“„ Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡:")
        st.text_area("Transcript", full_transcript[:3000], height=200)

        # ğŸ§  Step 7: Summarize transcript
        with st.spinner("ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†..."):
            summary = summarize_text(full_transcript)

        # ğŸ“ Step 8: Show summary
        st.subheader("ğŸ“ Ø®Ù„Ø§ØµÙ‡ Ù…ØªÙ†:")
        st.success(summary)

        # ğŸ§½ Cleanup
        os.remove(temp_path)


if __name__ == "__main__":
    main()
